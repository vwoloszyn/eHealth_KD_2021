{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
        }
      }
    },
    "colab": {
      "name": "flair.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDhrlrkXwdQk",
        "outputId": "45ad8aeb-06fe-45f7-a4ae-84414f79f4b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/vwoloszyn/eHealth_KD_2021\n",
        "!pip install flair\n",
        "import sys\n",
        "sys.path.append(\"./eHealth_KD_2021/scripts/\")\n",
        "from anntools import Collection\n",
        "from pathlib import Path\n",
        "from flair.data import Sentence, Corpus\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'eHealth_KD_2021' already exists and is not an empty directory.\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from flair) (0.0.6)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.6.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair) (5.9)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.7/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.95)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: gdown==3.12.2 in /usr/local/lib/python3.7/dist-packages (from flair) (3.12.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from flair) (1.2.12)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (4.2.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (0.0.43)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2020.12.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro7WKO5QwdQl"
      },
      "source": [
        "c = Collection()\n",
        "for fname in Path(\"./eHealth_KD_2021/2021/training/\").rglob(\"*.txt\"):\n",
        "    c.load(fname)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLAOZlLwdQm"
      },
      "source": [
        "#\n",
        "def brat2flair(brat):\n",
        "    out=[]\n",
        "    for s in brat:\n",
        "        sent=Sentence(s.text)\n",
        "        for t in sent:\n",
        "            for k in s.keyphrases:\n",
        "                if t.text in k.text:\n",
        "                    t.add_tag('ner', k.label)\n",
        "        out.append(sent)\n",
        "    return out\n",
        "\n",
        "x = np.split(c, [int(.8 * len(c)), int(.9 * len(c))])\n",
        "sentences_train=brat2flair(x[0])\n",
        "sentences_dev=brat2flair(x[1])\n",
        "sentences_test=brat2flair(x[2])\n",
        "corpus: Corpus = Corpus(sentences_train, sentences_dev, sentences_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "iNzkr7M7wdQm",
        "outputId": "473d91fc-f41c-4f0b-f619-43f64c871475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "print(tag_dictionary)\n",
        "\n",
        "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "\n",
        "embeddings = TransformerWordEmbeddings(\n",
        "    #model='distilbert-base-uncased',\n",
        "    model='xlm-roberta-base',\n",
        "    layers=\"-1\",\n",
        "    subtoken_pooling=\"first\",\n",
        "    fine_tune=True,\n",
        "    use_context=True,\n",
        ")\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "#tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "#                                        embeddings=embeddings,\n",
        "#                                        tag_dictionary=tag_dictionary,\n",
        "#                                        tag_type=tag_type,\n",
        "#                                         use_crf=True)\n",
        "\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger(\n",
        "    hidden_size=256,\n",
        "    embeddings=embeddings,\n",
        "    tag_dictionary=tag_dictionary,\n",
        "    tag_type='ner',\n",
        "    use_crf=False,\n",
        "    use_rnn=False,\n",
        "    reproject_embeddings=False,\n",
        ")\n",
        "\n",
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/ner-roberta-large',\n",
        "             learning_rate=5.0e-5,\n",
        "              mini_batch_size=4,\n",
        "              mini_batch_chunk_size=1,\n",
        "              max_epochs=5,\n",
        "              scheduler=OneCycleLR,\n",
        "              embeddings_storage_mode='none',\n",
        "              weight_decay=0.,\n",
        "              )\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary with 9 tags: <unk>, O, Concept, , Action, Reference, Predicate, <START>, <STOP>\n",
            "2021-03-18 14:57:09,104 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,107 Model: \"SequenceTagger(\n",
            "  (embeddings): TransformerWordEmbeddings(\n",
            "    (model): XLMRobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): RobertaEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): RobertaLayer(\n",
            "            (attention): RobertaAttention(\n",
            "              (self): RobertaSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): RobertaSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): RobertaIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): RobertaOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): RobertaPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (linear): Linear(in_features=768, out_features=9, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-03-18 14:57:09,108 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,113 Corpus: \"Corpus: 1200 train + 150 dev + 150 test sentences\"\n",
            "2021-03-18 14:57:09,122 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,125 Parameters:\n",
            "2021-03-18 14:57:09,126  - learning_rate: \"5e-05\"\n",
            "2021-03-18 14:57:09,130  - mini_batch_size: \"4\"\n",
            "2021-03-18 14:57:09,132  - patience: \"3\"\n",
            "2021-03-18 14:57:09,135  - anneal_factor: \"0.5\"\n",
            "2021-03-18 14:57:09,138  - max_epochs: \"5\"\n",
            "2021-03-18 14:57:09,141  - shuffle: \"True\"\n",
            "2021-03-18 14:57:09,142  - train_with_dev: \"False\"\n",
            "2021-03-18 14:57:09,147  - batch_growth_annealing: \"False\"\n",
            "2021-03-18 14:57:09,150 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,159 Model training base path: \"resources/taggers/ner-roberta-large\"\n",
            "2021-03-18 14:57:09,160 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,163 Device: cuda:0\n",
            "2021-03-18 14:57:09,164 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:09,166 Embeddings storage mode: none\n",
            "2021-03-18 14:57:09,176 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:57:25,381 epoch 1 - iter 30/300 - loss 1.07876405 - samples/sec: 7.41 - lr: 0.000050\n",
            "2021-03-18 14:57:41,498 epoch 1 - iter 60/300 - loss 0.90195661 - samples/sec: 7.45 - lr: 0.000050\n",
            "2021-03-18 14:57:57,151 epoch 1 - iter 90/300 - loss 0.79850827 - samples/sec: 7.67 - lr: 0.000050\n",
            "2021-03-18 14:58:12,231 epoch 1 - iter 120/300 - loss 0.76229020 - samples/sec: 7.96 - lr: 0.000049\n",
            "2021-03-18 14:58:27,362 epoch 1 - iter 150/300 - loss 0.71999152 - samples/sec: 7.93 - lr: 0.000049\n",
            "2021-03-18 14:58:42,564 epoch 1 - iter 180/300 - loss 0.69079393 - samples/sec: 7.90 - lr: 0.000048\n",
            "2021-03-18 14:58:57,770 epoch 1 - iter 210/300 - loss 0.67112635 - samples/sec: 7.89 - lr: 0.000048\n",
            "2021-03-18 14:59:12,907 epoch 1 - iter 240/300 - loss 0.64870291 - samples/sec: 7.93 - lr: 0.000047\n",
            "2021-03-18 14:59:28,028 epoch 1 - iter 270/300 - loss 0.63163286 - samples/sec: 7.94 - lr: 0.000046\n",
            "2021-03-18 14:59:43,149 epoch 1 - iter 300/300 - loss 0.62138319 - samples/sec: 7.94 - lr: 0.000045\n",
            "2021-03-18 14:59:43,151 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 14:59:43,152 EPOCH 1 done: loss 0.6214 - lr 0.0000452\n",
            "2021-03-18 14:59:46,499 DEV : loss 0.39788687229156494 - score 0.8214\n",
            "2021-03-18 14:59:46,503 BAD EPOCHS (no improvement): 4\n",
            "2021-03-18 14:59:46,509 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:00:02,034 epoch 2 - iter 30/300 - loss 0.56044925 - samples/sec: 7.73 - lr: 0.000044\n",
            "2021-03-18 15:00:17,407 epoch 2 - iter 60/300 - loss 0.51506789 - samples/sec: 7.81 - lr: 0.000043\n",
            "2021-03-18 15:00:32,717 epoch 2 - iter 90/300 - loss 0.49697222 - samples/sec: 7.84 - lr: 0.000042\n",
            "2021-03-18 15:00:48,104 epoch 2 - iter 120/300 - loss 0.48163794 - samples/sec: 7.80 - lr: 0.000041\n",
            "2021-03-18 15:01:03,489 epoch 2 - iter 150/300 - loss 0.48479300 - samples/sec: 7.80 - lr: 0.000040\n",
            "2021-03-18 15:01:18,912 epoch 2 - iter 180/300 - loss 0.47840102 - samples/sec: 7.78 - lr: 0.000038\n",
            "2021-03-18 15:01:34,289 epoch 2 - iter 210/300 - loss 0.47732862 - samples/sec: 7.81 - lr: 0.000037\n",
            "2021-03-18 15:01:49,630 epoch 2 - iter 240/300 - loss 0.46951604 - samples/sec: 7.82 - lr: 0.000036\n",
            "2021-03-18 15:02:05,145 epoch 2 - iter 270/300 - loss 0.46627429 - samples/sec: 7.74 - lr: 0.000034\n",
            "2021-03-18 15:02:20,591 epoch 2 - iter 300/300 - loss 0.46549285 - samples/sec: 7.77 - lr: 0.000033\n",
            "2021-03-18 15:02:20,593 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:02:20,594 EPOCH 2 done: loss 0.4655 - lr 0.0000328\n",
            "2021-03-18 15:02:23,864 DEV : loss 0.37750276923179626 - score 0.83\n",
            "2021-03-18 15:02:23,868 BAD EPOCHS (no improvement): 4\n",
            "2021-03-18 15:02:23,872 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:02:39,394 epoch 3 - iter 30/300 - loss 0.41801454 - samples/sec: 7.73 - lr: 0.000031\n",
            "2021-03-18 15:02:54,752 epoch 3 - iter 60/300 - loss 0.42121574 - samples/sec: 7.81 - lr: 0.000030\n",
            "2021-03-18 15:03:10,183 epoch 3 - iter 90/300 - loss 0.42116428 - samples/sec: 7.78 - lr: 0.000028\n",
            "2021-03-18 15:03:25,547 epoch 3 - iter 120/300 - loss 0.40833180 - samples/sec: 7.81 - lr: 0.000027\n",
            "2021-03-18 15:03:40,994 epoch 3 - iter 150/300 - loss 0.38771991 - samples/sec: 7.77 - lr: 0.000025\n",
            "2021-03-18 15:03:56,464 epoch 3 - iter 180/300 - loss 0.39239498 - samples/sec: 7.76 - lr: 0.000024\n",
            "2021-03-18 15:04:11,763 epoch 3 - iter 210/300 - loss 0.39165781 - samples/sec: 7.85 - lr: 0.000022\n",
            "2021-03-18 15:04:27,034 epoch 3 - iter 240/300 - loss 0.38868294 - samples/sec: 7.86 - lr: 0.000020\n",
            "2021-03-18 15:04:42,281 epoch 3 - iter 270/300 - loss 0.38445997 - samples/sec: 7.87 - lr: 0.000019\n",
            "2021-03-18 15:04:57,622 epoch 3 - iter 300/300 - loss 0.38401283 - samples/sec: 7.82 - lr: 0.000017\n",
            "2021-03-18 15:04:57,628 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:04:57,629 EPOCH 3 done: loss 0.3840 - lr 0.0000174\n",
            "2021-03-18 15:05:00,930 DEV : loss 0.39883142709732056 - score 0.8436\n",
            "2021-03-18 15:05:00,933 BAD EPOCHS (no improvement): 4\n",
            "2021-03-18 15:05:00,935 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:05:16,300 epoch 4 - iter 30/300 - loss 0.31047814 - samples/sec: 7.81 - lr: 0.000016\n",
            "2021-03-18 15:05:31,678 epoch 4 - iter 60/300 - loss 0.33373097 - samples/sec: 7.80 - lr: 0.000014\n",
            "2021-03-18 15:05:47,177 epoch 4 - iter 90/300 - loss 0.32980285 - samples/sec: 7.74 - lr: 0.000013\n",
            "2021-03-18 15:06:02,554 epoch 4 - iter 120/300 - loss 0.32140437 - samples/sec: 7.81 - lr: 0.000012\n",
            "2021-03-18 15:06:18,015 epoch 4 - iter 150/300 - loss 0.31044986 - samples/sec: 7.76 - lr: 0.000010\n",
            "2021-03-18 15:06:33,411 epoch 4 - iter 180/300 - loss 0.29703291 - samples/sec: 7.80 - lr: 0.000009\n",
            "2021-03-18 15:06:48,847 epoch 4 - iter 210/300 - loss 0.29022016 - samples/sec: 7.78 - lr: 0.000008\n",
            "2021-03-18 15:07:04,194 epoch 4 - iter 240/300 - loss 0.28232279 - samples/sec: 7.82 - lr: 0.000007\n",
            "2021-03-18 15:07:19,566 epoch 4 - iter 270/300 - loss 0.28960474 - samples/sec: 7.81 - lr: 0.000006\n",
            "2021-03-18 15:07:34,983 epoch 4 - iter 300/300 - loss 0.28574789 - samples/sec: 7.78 - lr: 0.000005\n",
            "2021-03-18 15:07:34,987 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:07:34,988 EPOCH 4 done: loss 0.2857 - lr 0.0000049\n",
            "2021-03-18 15:07:38,301 DEV : loss 0.41126543283462524 - score 0.8419\n",
            "2021-03-18 15:07:38,305 BAD EPOCHS (no improvement): 4\n",
            "2021-03-18 15:07:38,307 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:07:53,737 epoch 5 - iter 30/300 - loss 0.28162581 - samples/sec: 7.78 - lr: 0.000004\n",
            "2021-03-18 15:08:09,152 epoch 5 - iter 60/300 - loss 0.28753119 - samples/sec: 7.79 - lr: 0.000003\n",
            "2021-03-18 15:08:24,581 epoch 5 - iter 90/300 - loss 0.28322716 - samples/sec: 7.78 - lr: 0.000002\n",
            "2021-03-18 15:08:40,022 epoch 5 - iter 120/300 - loss 0.29427737 - samples/sec: 7.77 - lr: 0.000002\n",
            "2021-03-18 15:08:55,347 epoch 5 - iter 150/300 - loss 0.28628332 - samples/sec: 7.83 - lr: 0.000001\n",
            "2021-03-18 15:09:10,729 epoch 5 - iter 180/300 - loss 0.26374341 - samples/sec: 7.80 - lr: 0.000001\n",
            "2021-03-18 15:09:26,146 epoch 5 - iter 210/300 - loss 0.26377375 - samples/sec: 7.78 - lr: 0.000000\n",
            "2021-03-18 15:09:41,571 epoch 5 - iter 240/300 - loss 0.26370070 - samples/sec: 7.78 - lr: 0.000000\n",
            "2021-03-18 15:09:57,156 epoch 5 - iter 270/300 - loss 0.26292877 - samples/sec: 7.70 - lr: 0.000000\n",
            "2021-03-18 15:10:12,564 epoch 5 - iter 300/300 - loss 0.26380205 - samples/sec: 7.79 - lr: 0.000000\n",
            "2021-03-18 15:10:12,567 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:10:12,572 EPOCH 5 done: loss 0.2638 - lr 0.0000000\n",
            "2021-03-18 15:10:15,899 DEV : loss 0.4335750341415405 - score 0.8386\n",
            "2021-03-18 15:10:15,902 BAD EPOCHS (no improvement): 4\n",
            "2021-03-18 15:10:21,393 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-18 15:10:21,394 Testing using best model ...\n",
            "2021-03-18 15:10:24,844 \t0.8554\n",
            "2021-03-18 15:10:24,845 \n",
            "Results:\n",
            "- F-score (micro): 0.8231\n",
            "- F-score (macro): 0.7255\n",
            "- Accuracy (incl. no class): 0.8554\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Reference     0.6667    0.6667    0.6667        27\n",
            "      Action     0.8225    0.7026    0.7579       343\n",
            "     Concept     0.8823    0.8624    0.8722       930\n",
            "   Predicate     0.6154    0.5957    0.6054        94\n",
            "\n",
            "   micro avg     0.8462    0.8013    0.8231      1394\n",
            "   macro avg     0.7467    0.7069    0.7255      1394\n",
            "weighted avg     0.8454    0.8013    0.8221      1394\n",
            "\n",
            "2021-03-18 15:10:24,846 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [0.39788687229156494,\n",
              "  0.37750276923179626,\n",
              "  0.39883142709732056,\n",
              "  0.41126543283462524,\n",
              "  0.4335750341415405],\n",
              " 'dev_score_history': [0.8214, 0.83, 0.8436, 0.8419, 0.8386],\n",
              " 'test_score': 0.8231,\n",
              " 'train_loss_history': [0.6213831873734792,\n",
              "  0.46549285179159294,\n",
              "  0.3840128321914623,\n",
              "  0.28574789300289316,\n",
              "  0.2638020537192157]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9V9ayF8wdQn"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}